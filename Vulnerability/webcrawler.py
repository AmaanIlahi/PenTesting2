from django.shortcuts import render
from django.http import HttpResponse
import requests as req
import re
import urllib.parse

import os
from django.conf import settings


subdomain_list = []
subdir_list = []
target_links_list = []
url = ""

# Check if the site is responding
def requestf(URL):
    try:
        return req.get(URL)
    except req.exceptions.ConnectionError:
        pass


# Extracting all the hyperlinks from a website
def extract_links_from(target_url):
    response = req.request('GET', target_url)
    return re.findall(rb'(?:href=")(.*?)"', response.content)

def crawl(target_url):
    href_links = extract_links_from(target_url)
    for link in href_links:
        link = urllib.parse.urljoin(str(target_url), str(link)) #to expand the relative links

# To remove the tab links displayed separately, to avoid redundant links
        if "#" in link:
            link = link.split("#")[0]
        if target_url in link and link not in target_links_list:
# To print only the relevant links
            target_links_list.append(link)

    return target_links_list

def webcrawlhomepage(request):
    return render(request, 'webcrawler.html', context = {})

def index(request):
    #return HttpResponse("Welcome!!")
    global url
    if request.method == 'POST':
        url = request.POST["urllink"]

    #url = "https://zsecurity.org/"
    target_url = ""
    protocol_id1 = "http://"
    protocol_id2 = "https://"
    if protocol_id1 not in url and protocol_id2 not in url:
        target_url = "http://" + url
    else:
        target_url = url

    # Explore sub-domains
    wordlist_file = open(os.path.join(settings.BASE_DIR, 'Vulnerability/referfiles/wordlist.txt'))
    #with open("wordlist.txt", "r") as wordlist_file:
    for line in wordlist_file:
        word = line.strip()
        if protocol_id1 in target_url:
            temp = target_url.split("http://")[1]
        else:
            temp = target_url.split("https://")[1]

        test_url = "http://" + word + "." + temp

        response = requestf(test_url)
        if response:
            subdomain_list.append(test_url)
                #print("[+] Discovered Subdomain --> " + test_url)
    #print(subdomain_list, sep = "\n")
    print ("______________________________________________________________")


    # Explore sub-directories
    subdir_file = open(os.path.join(settings.BASE_DIR, 'Vulnerability/referfiles/common.txt'))
    #with open("common.txt", "r") as subdir_file:
    for line in subdir_file:
        word = line.strip()
        test_url = target_url + "/" + word
        response = requestf(test_url)
        if response:
            subdir_list.append(test_url)
                #print("[+] Discovered URL --> " + test_url)
    #print(subdir_list, sep = "\n")
    print ("______________________________________________________________")


    target_links_list = crawl(target_url)
    return render(request, 'webcrawlresults.html', {'list1':subdomain_list, 'list2':subdir_list, 'list3':target_links_list})
